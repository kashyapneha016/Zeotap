{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIWwtKWJ2z2o",
        "outputId": "09b2f575-d935-4a09-a4da-86ce1262b5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Import necessary libraries"
      ],
      "metadata": {
        "id": "F6qCvM00KES1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "e6Pz1h802_W4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "dw3-96WlKGi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers=pd.read_csv(\"/content/drive/MyDrive/Assig/Customers.csv\")"
      ],
      "metadata": {
        "id": "4V93wMBC3ACK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products = pd.read_csv('/content/drive/MyDrive/Assig/Products.csv')"
      ],
      "metadata": {
        "id": "aX9e-mjv3ES3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = pd.read_csv('/content/drive/MyDrive/Assig/Transactions.csv')"
      ],
      "metadata": {
        "id": "p4MUY6sU3HcM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "_W2Jf7ekLJJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge 'transactions' with 'products' dataframe to include the product category\n",
        "transactions = transactions.merge(products[['ProductID', 'Category']], on='ProductID', how='left')"
      ],
      "metadata": {
        "id": "z3I_We6u3Y-8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding of Product Categories\n",
        "category_dummies = pd.get_dummies(transactions['Category'])\n",
        "transactions = pd.concat([transactions, category_dummies], axis=1)"
      ],
      "metadata": {
        "id": "ke9H-IVm4F0A"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate transaction features for each customer\n",
        "# Sum of the one-hot encoded product category values by customer to create a feature vector\n",
        "customer_profiles = transactions.groupby('CustomerID').agg({\n",
        "    cat: 'sum' for cat in category_dummies.columns\n",
        "}).reset_index()"
      ],
      "metadata": {
        "id": "5D6-OzZw4MXj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize customer profiles by scaling values\n",
        "customer_profiles_normalized = customer_profiles.iloc[:, 1:].apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=0)"
      ],
      "metadata": {
        "id": "ottf4kfL4NXp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate similarity scores"
      ],
      "metadata": {
        "id": "NAHtf0WpPSQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate similarity scores (cosine similarity)\n",
        "cosine_sim = cosine_similarity(customer_profiles_normalized)"
      ],
      "metadata": {
        "id": "GxugN-HQ4Rfk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommend top 3 lookalikes for each customer (C0001 - C0020)\n",
        "lookalike_dict = {}\n",
        "\n",
        "for customer_id in customers['CustomerID'][:20]:  # for customers C0001 to C0020\n",
        "    customer_index = customers[customers['CustomerID'] == customer_id].index[0]\n",
        "\n",
        "    # Get similarity scores with other customers\n",
        "    similarity_scores = list(enumerate(cosine_sim[customer_index]))\n",
        "\n",
        "    # Sort customers by similarity score (descending) and exclude the customer itself\n",
        "    sorted_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get top 3 lookalikes (excluding the first one as it's the customer itself)\n",
        "    top_lookalikes = sorted_scores[1:4]\n",
        "\n",
        "    # Store the result in the dictionary\n",
        "    lookalike_dict[customer_id] = [(customers.iloc[i[0]]['CustomerID'], i[1]) for i in top_lookalikes]\n"
      ],
      "metadata": {
        "id": "G1yoGJ5J4Un3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Lookalike Recommendations to CSV"
      ],
      "metadata": {
        "id": "xzk1YU2aPFsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Lookalike.csv file\n",
        "lookalike_df = pd.DataFrame(columns=['CustomerID', 'LookalikeCustomerID', 'SimilarityScore'])\n",
        "\n",
        "for customer_id, lookalikes in lookalike_dict.items():\n",
        "    for lookalike, score in lookalikes:\n",
        "        # Create a temporary DataFrame for the new row\n",
        "        new_row = pd.DataFrame({'CustomerID': [customer_id], 'LookalikeCustomerID': [lookalike], 'SimilarityScore': [score]})\n",
        "        # Concatenate the new row with the existing DataFrame\n",
        "        lookalike_df = pd.concat([lookalike_df, new_row], ignore_index=True)\n",
        "\n",
        "# Save the lookalikes to a CSV file\n",
        "lookalike_df.to_csv('Neha_Lookalike.csv', index=False)\n",
        "\n",
        "print(\"Lookalike model completed and saved to Lookalike.csv.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfI4bk2Z4lMd",
        "outputId": "36077973-bc39-4153-bb9c-a43d711a9de2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lookalike model completed and saved to Lookalike.csv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-bf7f5dfaaee6>:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  lookalike_df = pd.concat([lookalike_df, new_row], ignore_index=True)\n"
          ]
        }
      ]
    }
  ]
}